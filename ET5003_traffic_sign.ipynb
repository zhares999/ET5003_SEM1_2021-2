{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ET5003_traffic_sign.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KlZ35vXzY-p"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
        "</div> \n",
        "\n",
        "#**Artificial Intelligence - MSc**\n",
        "ET5003 - MACHINE LEARNING APPLICATIONS \n",
        "\n",
        "###Instructor: Enrique Naredo\n",
        "###ET5003_traffic_sign"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4TXn9dKIWAU"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhwC4auynd4n"
      },
      "source": [
        "[Classification](https://towardsdatascience.com/machine-learning-classifiers-a5cc4e1b0623) is the process of predicting the class of given data points.\n",
        "\n",
        "- An easy to understand example is classifying emails as “spam” or “not spam.”\n",
        "- In machine learning an algorithm learns how to assign a class label to examples from a problem domain.\n",
        "- Classification belongs to the category of supervised learning where the targets also provided with the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS9sUl2A1v6Y"
      },
      "source": [
        "In this notebook we will solve a classification problem using the well-known Mnist dataset and the also well-known classifier algorithm Logistic Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hhnCK3LyyYH"
      },
      "source": [
        "# Traffic Sign Recognition Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg7VCbX77eAA"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFFLThrpwibd"
      },
      "source": [
        "# Suppressing Warnings:\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFl-x4SCjlIi"
      },
      "source": [
        "! pip install opencv-python\n",
        "! pip install scikit-image\n",
        "! pip install arviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymyi02zi7h2l"
      },
      "source": [
        "from scipy.io import loadmat\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline\n",
        "import pymc3 as pm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import HTML\n",
        "import random\n",
        "import pickle\n",
        "import arviz as az\n",
        "import theano as tt\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.utils import shuffle\n",
        "from skimage.color import rgb2gray\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-3vBYgtjlIh"
      },
      "source": [
        "## Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBEXO3tJ4-TV"
      },
      "source": [
        "The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits.\n",
        "\n",
        "- The MNIST database contains 60,000 training images and 10,000 testing images.\n",
        "- An extended dataset similar to MNIST called EMNIST has been published in 2017, which contains 240,000 training images, and 40,000 testing images of handwritten digits and characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KW7aDkkjHJc"
      },
      "source": [
        "# Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m5FEFiiwa9X"
      },
      "source": [
        "1. [Download the dataset](https://d17h27t6h515a5.cloudfront.net/topher/2016/November/581faac4_traffic-signs-data/traffic-signs-data.zip). This is a pickled dataset with size \n",
        "32x32 images.\n",
        "\n",
        "2. Upload the dataset into your folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TptzG7Z9zk-b"
      },
      "source": [
        "How to upload data to google colab: [here.](https://www.youtube.com/watch?v=-1jFfadz7bo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGbH8-DHaIM8"
      },
      "source": [
        "At the right hand of this notebook you will find 4 icons:\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1G-nwwqYv-KeLFuBkIbVgw_fiNm9RhluK\" width=\"250\"/>\n",
        "</div>\n",
        "\n",
        "Follow this steps:\n",
        "* click on the folder icon\n",
        "* choose one of the existing folders or create a new one\n",
        "* click on the three vertical points \n",
        "* choose 'upload'\n",
        "* upload your file\n",
        "\n",
        "\n",
        "Once you are done, in the following cell, paste the path to that file similar to this example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8j6snEnJ1Lmx"
      },
      "source": [
        "# paste the path here, keep quotation marks and the trailing slash\n",
        "Path = '/content/drive/MyDrive/Colab Notebooks/Enrique/Data/traffic-signs-data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rosmH4665uJ"
      },
      "source": [
        "# training dataset: train.p\n",
        "training_file = Path+'train.p'\n",
        "# test dataset: test.p\n",
        "testing_file = Path+'test.p'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqo01fXrxIPA"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yeZsriLjlIi"
      },
      "source": [
        "### Preprocess the data here. \n",
        "## Preprocessing steps could include normalization, converting to grayscale, etc.\n",
        "\n",
        "def normalize(img):\n",
        "    return cv2.normalize(img, img, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    \n",
        "\n",
        "def grayscale(img):\n",
        "    #COLOR_BGR2GRAY COLOR_RGB2GRAY\n",
        "    return rgb2gray(img)\n",
        "    #return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  \n",
        "\n",
        "def setgrayscale(images):\n",
        "    #from rgb to gray\n",
        "    print(images.shape)\n",
        "    result = np.zeros(shape=(len(images),32,32))\n",
        "    for i in range(len(images)): \n",
        "        #gray = np.resize(grayscale(images[i]), (32, 32, 1))\n",
        "        gray_img = grayscale(images[i])\n",
        "        img_normalized = normalize(gray_img)\n",
        "        result[i] = gray_img\n",
        "    print(result.shape)\n",
        "    return result\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH_5ez5TjlIj"
      },
      "source": [
        "# open training and testing files\n",
        "\n",
        "with open(training_file, mode='rb') as f:\n",
        "    train = pickle.load(f)\n",
        "    \n",
        "with open(testing_file, mode='rb') as f:\n",
        "    test = pickle.load(f)\n",
        "\n",
        "# assigning features and labels    \n",
        "X_train, y_train = train['features'], train['labels']\n",
        "X_test, y_test = test['features'], test['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0G_V51BhPnG"
      },
      "source": [
        "## Build a Traffic Sign Recognition Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g19OkjsMjlIb"
      },
      "source": [
        "We will use a multinomial logistic regression. We extend the example we saw in the Challenger Disaster to deal with more than two classes.\n",
        "\n",
        "Let us assume we have k classes. Specifically, in multinomial logistic regression, we have a different linear model\n",
        "$$\n",
        "\\alpha_j+\\beta_j^T x\n",
        "$$\n",
        "for each class $j=1,\\dots,k$.\n",
        "\n",
        "The likelihood is then\n",
        "$$\n",
        "p(y=j | \\theta, x) = \\frac{\\exp(\\alpha_j+\\beta_{j}^T x)}{\\sum_{c=1}^k\\exp(\\alpha_c+\\beta_c^Tx)}.\n",
        "$$\n",
        "with $\\theta=[\\alpha_1,\\beta_1,\\alpha_2,\\beta_{2},\\alpha_3,\\beta_{3},\\dots,\\alpha_k,\\beta_{k}]$ is the vector of all parameters.\n",
        "\n",
        "Let's consider the case k=2 as an example then\n",
        "\n",
        "$$\n",
        "p(y=0 | \\theta, x) = \\frac{\\exp(\\alpha_1+\\beta_1^Tx)}{\\exp(\\alpha_1+\\beta_1^Tx)+\\exp(\\alpha_2+\\beta_2^Tx)}.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "p(y=1 | \\theta, x) = \\frac{\\exp(\\alpha_2+\\beta_2^Tx)}{\\exp(\\alpha_1+\\beta_1^Tx)+\\exp(\\alpha_2+\\beta_2^Tx)}.\n",
        "$$\n",
        "\n",
        "In order to recover the binomial logistic regression, we need to assume that $\\alpha_1=0$ and $\\beta_1=0$:\n",
        "$$\n",
        "p(y=0 | \\theta, x) = \\frac{1}{1+\\exp(\\alpha_2+\\beta_2^Tx)}.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "p(y=1 | \\theta, x) = \\frac{\\exp(\\alpha_2+\\beta_2^Tx)}{1+\\exp(\\alpha_2+\\beta_2^Tx)}.\n",
        "$$\n",
        "and we obtain the binomial logistic regression.\n",
        "We always assume that $\\alpha_1=0$ and $\\beta_1=0$ even in the case $k>2$, in this way the model is a proper\n",
        "extension of the binomial logistic regression.\n",
        "\n",
        "\n",
        "Thus, for $k=3$, we have that\n",
        "$$\n",
        "p(y=0 | \\theta, x) = \\frac{1}{1+\\exp(\\alpha_2+\\beta_2^Tx)+\\exp(\\alpha_3+\\beta_3^Tx)}.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "p(y=1 | \\theta, x) = \\frac{\\exp(\\alpha_2+\\beta_2^Tx)}{1+\\exp(\\alpha_2+\\beta_2^Tx)+\\exp(\\alpha_3+\\beta_3^Tx)}.\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "p(y=2 | \\theta, x) = \\frac{\\exp(\\alpha_3+\\beta_3^Tx)}{1+\\exp(\\alpha_2+\\beta_2^Tx)+\\exp(\\alpha_3+\\beta_3^Tx)}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zofvMI7mjlIj"
      },
      "source": [
        "---\n",
        "\n",
        "## Step 1: Dataset Summary \n",
        "\n",
        "The pickled data is a dictionary with 4 key/value pairs, but the useful ones are:\n",
        "\n",
        "- `'features'` is a 4D array containing raw pixel data of the traffic sign images, (num examples, width, height, channels).\n",
        "- `'labels'` is a 2D array containing the label/class id of the traffic sign. The file `signnames.csv` contains id -> name mappings for each id.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5OaHLTGjlIj"
      },
      "source": [
        "# Number of training examples\n",
        "n_train =  len(X_train)\n",
        "\n",
        "# Number of testing examples.\n",
        "n_test = len(X_test)\n",
        "\n",
        "#  shape of an traffic sign image\n",
        "image_shape =X_train[0].shape\n",
        "\n",
        "# unique classes/labels in the dataset.\n",
        "alltotal = set(y_train )\n",
        "\n",
        "# number of classes\n",
        "n_classes = len(alltotal )\n",
        "\n",
        "# print information\n",
        "print(\"Number of training examples =\", n_train)\n",
        "print(\"Number of testing examples =\", n_test)\n",
        "print(\"Image data shape =\", image_shape)\n",
        "print(\"Number of classes =\", n_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0WqVS2pjlIk"
      },
      "source": [
        "# setup figure to show images; 2 rows, 5 cols\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "# setup spaces between images\n",
        "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "## show  10 random images\n",
        "# ToDo: assure to don't chose same image twice\n",
        "for i in range(10):\n",
        "    # take index randomly chosen\n",
        "    index = random.randint(0, len(X_train))\n",
        "    # choose the image according to the index\n",
        "    image = X_train[index]\n",
        "    axs[i].axis('off')\n",
        "    # show the image\n",
        "    axs[i].imshow(image)\n",
        "    # give a meaningful title\n",
        "    axs[i].set_title('image #' + str(y_train[index]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ctudHes1ylw"
      },
      "source": [
        "## Imbalanced dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2QaWOXXjlIl"
      },
      "source": [
        "The bar chart shows the frequency distribution of traffic sign respect to the classes.\n",
        " \n",
        "* Some classes are much more frequent than others.\n",
        "* This shows we are dealing with an imbalanced dataset.\n",
        "* This may be a problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqrB5x-1jlIl"
      },
      "source": [
        "## plot histogram\n",
        "fig, ax = plt.subplots()\n",
        "# array with evenly spaced classes\n",
        "ind = np.arange(n_classes)\n",
        "\n",
        "# histogram\n",
        "n, bins, patches = ax.hist(y_train, n_classes)\n",
        "# horizontal axis label\n",
        "ax.set_xlabel('classes')\n",
        "# vertical axis label\n",
        "ax.set_ylabel('counts')\n",
        "# plot title\n",
        "ax.set_title(r'Histogram of traffic sign images')\n",
        "# show plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3_N6x7TjlIm"
      },
      "source": [
        "----\n",
        "\n",
        "## Step 2: Data Preparation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-PxM976jlIm"
      },
      "source": [
        "We first convert the images in grayscale so to simplify the problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muOnVwBYjlIm"
      },
      "source": [
        "# convert to grayscale\n",
        "X_traingray = setgrayscale(X_train)\n",
        "X_testgray = setgrayscale(X_test)\n",
        "\n",
        "# random index\n",
        "index = random.randint(0, len(X_traingray))\n",
        "\n",
        "#original image\n",
        "image = X_train[index]\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(image)\n",
        "\n",
        "#grayscale\n",
        "image = X_traingray[index]\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(image,cmap=\"Greys_r\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFF_DfftjlIm"
      },
      "source": [
        "The images have also been normalized to avoid numerical problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PxYsiS3jlIm"
      },
      "source": [
        "We select only three classes and down-sample the dataset to reduce inference computational time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRdWtIICjlIn"
      },
      "source": [
        "np.random.seed(0)\n",
        "#classes = [1, 2, 4]\n",
        "classes = [3, 7, 9]\n",
        "N_per_class =500\n",
        "\n",
        "X = []\n",
        "labels = []\n",
        "for d in classes:\n",
        "    imgs = X_traingray[np.where(y_train==d)[0],:]\n",
        "    X.append(imgs[np.random.permutation(imgs.shape[0]),:][0:N_per_class,:])\n",
        "    labels.append(np.ones(N_per_class)*d)\n",
        "X_traingray2 = np.vstack(X).astype(np.float64)\n",
        "y_train2 = np.hstack(labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt9VDfAAjlIn"
      },
      "source": [
        "print(X_traingray2.shape,y_train2.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPRJ_hD1jlIn"
      },
      "source": [
        "### we split the dataset in training and validation\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_traingray2, y_train2, test_size=0.2, random_state=0)\n",
        " \n",
        "\n",
        "X_tr, y_tr = shuffle(X_tr, y_tr)\n",
        "\n",
        "print(X_tr.shape)\n",
        "print(X_val.shape)\n",
        "print(y_tr.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSceDzx3jlIo"
      },
      "source": [
        "# transform images into vectors  \n",
        "X_trv = X_tr.flatten().reshape(X_tr.shape[0],X_tr.shape[1]*X_tr.shape[2])\n",
        "X_valv = X_val.flatten().reshape(X_val.shape[0],X_tr.shape[1]*X_tr.shape[2])\n",
        "print(X_trv.shape)\n",
        "print(X_valv.shape)\n",
        "print(y_tr.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmIHhmYld6lB"
      },
      "source": [
        "## Step 3: Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fen7Tr75jlIo"
      },
      "source": [
        "There are various aspects to consider when thinking about this problem:\n",
        "\n",
        "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
        "- Number of examples per label (some have more than others).\n",
        "- data augmentation to increase the dataset.\n",
        "\n",
        "For now, we are not interested in obtaining the highest performance, but more in understanding the various approaches.\n",
        "\n",
        "We define our \"general recipe\" machine learning model: logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOoLiOiIjlIo"
      },
      "source": [
        "#General-recipe ML logistic regression\n",
        "clf = LogisticRegression(random_state=0, max_iter=2000, C=100, solver='lbfgs', multi_class='multinomial').fit(X_trv, y_tr)\n",
        "y_pred_logi = clf.predict(X_valv)\n",
        "y_pred_logi_prob = clf.predict_proba(X_valv)\n",
        "prob_classmax = np.max(y_pred_logi_prob,axis=1)\n",
        "print(\"Accuracy=\", accuracy_score(y_pred_logi,y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grjXa4y1jlIp"
      },
      "source": [
        "np.sort(prob_classmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjZHdd7GjlIp"
      },
      "source": [
        "# probability of general-recipe logistic regression in wrong instances\n",
        "prob_classmax[y_pred_logi!=y_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky7ZsGjVjlIp"
      },
      "source": [
        "it performs quite well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pai55_sRjlIp"
      },
      "source": [
        "np.array(y_pred_logi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zLAHA9fjlIp"
      },
      "source": [
        "## Step 4: Probabilistic ML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fnYiD1CjlIq"
      },
      "source": [
        "import sklearn.preprocessing\n",
        "## We use LabelBinarizer to transfor classes into counts\n",
        "# neg_label=0, pos_label=1 \n",
        "y_2_bin = sklearn.preprocessing.LabelBinarizer().fit_transform(y_tr.reshape(-1,1))\n",
        "nf = X_trv.shape[1]\n",
        "# number of classes\n",
        "nc = len(classes)\n",
        "# floatX = float32\n",
        "floatX = tt.config.floatX\n",
        "\n",
        "init_b = np.random.randn(nf, nc-1).astype(floatX)\n",
        "init_a = np.random.randn(nc-1).astype(floatX)\n",
        "\n",
        "\n",
        "with pm.Model() as multi_logistic:\n",
        "    # Prior\n",
        "    β = pm.Normal('beta', 0, sigma=100, shape=(nf, nc-1), testval=init_b)\n",
        "    α = pm.Normal('alpha', 0, sigma=100, shape=(nc-1,), testval=init_a)\n",
        "    \n",
        "    # we need to consider nc-1 features because the model is not identifiable\n",
        "    # the softmax turns a vector into a probability that sums up to one\n",
        "    # therefore we add zeros to go back to dimension nc\n",
        "    # so that softmax returns a vector of dimension nc\n",
        "    β1  = tt.tensor.concatenate([np.zeros((nf,1)),β ],axis=1)\n",
        "    α1  = tt.tensor.concatenate([[0],α ],)\n",
        "    \n",
        "    # Likelihood\n",
        "    mu = pm.math.matrix_dot(X_trv,β1) + α1\n",
        "    # It doesn't work if the problem is binary\n",
        "    p = tt.tensor.nnet.nnet.softmax(mu)\n",
        "    observed = pm.Multinomial('likelihood', p=p, n=1, observed=y_2_bin)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UJwo4SEOMzi"
      },
      "source": [
        "y_2_bin\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLfbb1ufjlIq"
      },
      "source": [
        "with multi_logistic:\n",
        "    #approx = pm.fit(300000, method='advi') # takes longer\n",
        "    approx = pm.fit(3000, method='advi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGHaxwdljlIq"
      },
      "source": [
        "plt.plot(approx.hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO6py8pnjlIr"
      },
      "source": [
        "dd = 300\n",
        "posterior = approx.sample(draws=dd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsIb_0aOjlIr"
      },
      "source": [
        "## The softmax function transforms each element of a collection by computing the exponential\n",
        "#  of each element divided by the sum of the exponentials of all the elements.\n",
        "from scipy.special import softmax\n",
        "\n",
        "#select an image in the test set\n",
        "i = 10 \n",
        "#i = random.randint(0, dd)\n",
        "\n",
        "#select a sample in the posterior\n",
        "s = 100 \n",
        "#s = random.randint(0, dd)\n",
        "\n",
        "\n",
        "beta  = np.hstack([np.zeros((nf,1)),  posterior['beta'][s,:] ])\n",
        "alpha = np.hstack([[0],  posterior['alpha'][s,:] ])\n",
        "image = X_valv[i,:].reshape(32,32)\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(image,cmap=\"Greys_r\")\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "print(\"test image #\" + str(i))\n",
        "print(\"posterior sample #\" + str(s))\n",
        "print(\"true class=\", y_val[i])\n",
        "print(\"classes: \" + str(classes))\n",
        "print(\"estimated prob=\",softmax((np.array([X_valv[i,:].dot(beta) + alpha])))[0,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEoqLyo6jlIr"
      },
      "source": [
        "# Bayesian prediction\n",
        "# return the class that has the highest posterior probability\n",
        "y_pred_Bayesian=[]\n",
        "\n",
        "for i in range(X_valv.shape[0]):\n",
        "    val=np.zeros((1,len(classes)))\n",
        "    \n",
        "    for s in range(posterior['beta'].shape[0]):\n",
        "        beta = np.hstack([np.zeros((nf,1)),  posterior['beta'][s,:] ])\n",
        "        alpha = np.hstack([[0],  posterior['alpha'][s,:] ])     \n",
        "        val = val + softmax((np.array([X_valv[i,:].dot(beta) + alpha])))\n",
        "    \n",
        "    mean_probability = val/posterior['beta'].shape[0]\n",
        "    y_pred_Bayesian.append( np.argmax(mean_probability))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYBwhQYtYEfD"
      },
      "source": [
        "print(y_pred_Bayesian)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5zM4NpqYWUP"
      },
      "source": [
        "# recall the classes we are using\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X59ZLa-wjlIr"
      },
      "source": [
        "# prediction array (using classes)\n",
        "nn = 10 # just an example\n",
        "np.array(classes)[y_pred_Bayesian[0:nn]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmOq8282jlIr"
      },
      "source": [
        "# using validation: y_val\n",
        "print(\"Accuracy=\", accuracy_score(np.array(classes)[y_pred_Bayesian], y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G31SnW6rjlIs"
      },
      "source": [
        "## Selecting different instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvxjLnytjlIs"
      },
      "source": [
        "y_predB=[]\n",
        "\n",
        "for i in range(X_valv.shape[0]):\n",
        "    #print(i)\n",
        "    val=[]\n",
        "\n",
        "    for s in range(posterior['beta'].shape[0]):\n",
        "        beta = np.hstack([np.zeros((nf,1)),  posterior['beta'][s,:] ])\n",
        "        alpha = np.hstack([[0],  posterior['alpha'][s,:] ])\n",
        "        val.append(softmax((np.array([X_valv[i,:].dot(beta) + alpha])))[0,:])\n",
        "\n",
        "    #mean probability\n",
        "    valmean = np.mean(val,axis=0)\n",
        "    #class with maximum mean probability\n",
        "    classmax = np.argmax(valmean)\n",
        "    #ranks\n",
        "    ranks = np.array(val.copy())\n",
        "    ranks   = ranks  *0 #init\n",
        "    colmax = np.argmax(np.array(val),axis=1)\n",
        "    ranks[np.arange(0,len(colmax)),colmax]=1\n",
        "     \n",
        "    y_predB.append( [classmax, valmean[classmax], np.std(ranks,axis=0)[classmax]])\n",
        "\n",
        "    \n",
        "y_predB= np.array(y_predB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnEeHwFPjlIs"
      },
      "source": [
        "# prediction array\n",
        "mm = 10\n",
        "y_predB[0:mm,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgwzPtDWjlIt"
      },
      "source": [
        "#sorting in descending order\n",
        "difficult = np.argsort(-y_predB[:,2])\n",
        "y_predB[difficult[0:mm],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGtaG6rDjlIt"
      },
      "source": [
        "#probability of general-recipe logistic regression in wrong instances\n",
        "prob_classmax[y_pred_logi != y_val]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLVDYSdejlIt"
      },
      "source": [
        "y_predB[y_pred_logi != y_val,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZstODCDjlIt"
      },
      "source": [
        "## Difficult & easy instances\n",
        "\n",
        "easy = np.argsort(y_predB[:,2])\n",
        "print(\"Accuracy in easy instances =\", accuracy_score(y_pred_logi[easy[0:100]], y_val[easy[0:100]]))\n",
        "\n",
        "difficult = np.argsort(-y_predB[:,2])\n",
        "print(\"Accuracy in difficult instances =\", accuracy_score(y_pred_logi[difficult[0:100]], y_val[difficult[0:100]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_i_0Kr9jlIt"
      },
      "source": [
        "# show 10 random 'easy' images\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "\n",
        "for i in range(10):\n",
        "    index = easy[i]\n",
        "    image = X_valv[index,:].reshape(32,32)\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(image,cmap=\"Greys_r\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2sxXDYjlIu"
      },
      "source": [
        "\n",
        "# show 10 random 'difficult' images\n",
        "fig, axs = plt.subplots(2,5, figsize=(15, 6))\n",
        "fig.subplots_adjust(hspace = .2, wspace=.001)\n",
        "axs = axs.ravel()\n",
        "for i in range(10):\n",
        "    index = difficult[i]\n",
        "    image = X_valv[index,:].reshape(32,32)\n",
        "    axs[i].axis('off')\n",
        "    axs[i].imshow(image,cmap=\"Greys_r\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}