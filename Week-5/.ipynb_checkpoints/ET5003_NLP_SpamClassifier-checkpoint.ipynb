{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "930vlW5BrOtq"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1vK33e_EqaHgBHcbRV_m38hx6IkG0blK_\" width=\"350\"/>\n",
    "</div> \n",
    "\n",
    "#**Artificial Intelligence - MSc**\n",
    "##ET5003 - MACHINE LEARNING APPLICATIONS \n",
    "\n",
    "###Instructor: Enrique Naredo\n",
    "###ET5003_NLP_SpamClasiffier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQyc5Xgdj7W9"
   },
   "source": [
    "\n",
    "### Spam Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1z8j0PtO77Xs"
   },
   "source": [
    "[Spamming](https://en.wikipedia.org/wiki/Spamming) is the use of messaging systems to send multiple unsolicited messages (spam) to large numbers of recipients for the purpose of commercial advertising, for the purpose of non-commercial proselytizing, for any prohibited purpose (especially the fraudulent purpose of phishing), or simply sending the same message over and over to the same user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_fap_t2DrME"
   },
   "source": [
    "Spam Classification: Deciding whether an email is spam or not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg7VCbX77eAA"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k96-GLUGE2ux"
   },
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxMYIOx1FONV"
   },
   "outputs": [],
   "source": [
    "# Scikit-learn is an open source machine learning library \n",
    "# that supports supervised and unsupervised learning\n",
    "# https://scikit-learn.org/stable/\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLlFHWx4j7W6"
   },
   "outputs": [],
   "source": [
    "# Regular expression operations\n",
    "#https://docs.python.org/3/library/re.html\n",
    "import re \n",
    "\n",
    "# Natural Language Toolkit\n",
    "# https://www.nltk.org/install.html\n",
    "import nltk\n",
    "\n",
    "# Stemming maps different forms of the same word to a common “stem” \n",
    "# https://pypi.org/project/snowballstemmer/\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# https://www.nltk.org/book/ch02.html\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4iXKNAGj7W_"
   },
   "source": [
    "## Step 1: Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NY-KIbRkMMJ"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5akGy2kSj7W_"
   },
   "outputs": [],
   "source": [
    "# path to your (local/cloud) drive \n",
    "path = '/content/drive/MyDrive/Colab Notebooks/Enrique/Data/spam/'\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(path+'spam.csv', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DkrYoNn3JHxe"
   },
   "outputs": [],
   "source": [
    "# original dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MODwpwEkJGky"
   },
   "outputs": [],
   "source": [
    "# remove useless features\n",
    "df = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDvr4HpKGZlN"
   },
   "outputs": [],
   "source": [
    "# v1 -> is the class label: ham, spam\n",
    "# ham -> https://en.wiktionary.org/wiki/ham_e-mail\n",
    "# spam -> https://en.wiktionary.org/wiki/spam#English\n",
    "# v2 -> is the  email\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMyuPYcuj7XB"
   },
   "source": [
    "## Step 2: Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmTe13CNKS_s"
   },
   "outputs": [],
   "source": [
    "# Removing stopwords and stemming\n",
    "# a stem must be a word\n",
    "# Example:  fishing, fished, and fisher: stem -> fish\n",
    "# choose English as the target language\n",
    "stemmer = SnowballStemmer('english', ignore_stopwords=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNG-xh8wMOTi"
   },
   "outputs": [],
   "source": [
    "# Stop words are basically a set of commonly used words in any language\n",
    "# https://en.wikipedia.org/wiki/Stop_word\n",
    "# and are filtered out before processing of natural language data \n",
    "# Example list: https://github.com/igorbrigadir/stopwords/blob/master/en/terrier.txt\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3mbvmBFO310"
   },
   "outputs": [],
   "source": [
    "# example: remove anything that is not a letter\n",
    "string_sample = '123This @45is 890-130 an_example !!'\n",
    "new_string = re.sub('[^a-zA-Z]', ' ', string_sample) \n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "riD32a4VSQPb"
   },
   "outputs": [],
   "source": [
    "# removing duplicated spaces\n",
    "\" \".join(new_string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF15oylkN_UD"
   },
   "outputs": [],
   "source": [
    "# remove anything that is not a letter in the emails\n",
    "df['v2'] = [re.sub('[^a-zA-Z]', ' ', sms) for sms in df['v2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ujye-Z_GNcKm"
   },
   "outputs": [],
   "source": [
    "# list of words in the emails\n",
    "email_words = [sms.split() for sms in df['v2']]\n",
    "print(email_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCzsxUkXj7XB"
   },
   "outputs": [],
   "source": [
    "# function to normalize words\n",
    "def normalize(words):\n",
    "  normalized_words = list()\n",
    "  for word in words:\n",
    "    # remove  the most common words\n",
    "    if word.lower() not in stop: \n",
    "      # stemming\n",
    "      new_word = stemmer.stem(word) \n",
    "      # lower case\n",
    "      normalized_words.append(new_word.lower()) \n",
    "  return normalized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACEku-N6Tag9"
   },
   "outputs": [],
   "source": [
    "# normalize words in emails\n",
    "email_words_norm = [normalize(word) for word in email_words]\n",
    "print(email_words_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBNa4IlTgUef"
   },
   "outputs": [],
   "source": [
    "# update dataframe\n",
    "df['v2'] = [\" \".join(word) for word in email_words_norm]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUSE99a6j7XC"
   },
   "outputs": [],
   "source": [
    "# training and test datasets\n",
    "data_register = df['v2']\n",
    "class_label = df['v1']\n",
    "factor = 0.2\n",
    "lucky_number = 7\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_register, class_label, test_size=factor, random_state=lucky_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7N04_o7iVCe"
   },
   "outputs": [],
   "source": [
    "# train\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0_UqNlqi62U"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttq_X1YHjY1p"
   },
   "outputs": [],
   "source": [
    "# class labels in training\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AD7KRHWsj7XC"
   },
   "outputs": [],
   "source": [
    "# reshape -> gives a new shape to an array without changing its data\n",
    "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.reshape.html#numpy.reshape\n",
    "# -1 -> the unspecified value is inferred\n",
    "y_train.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8NL5aZ4fb0T"
   },
   "source": [
    "**Build your custome function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USUzEMJSW5WL"
   },
   "source": [
    "The real difference between stemming and lemmatization is that stemming reduces word-forms to (pseudo) stems which might be meaningful or meaningless, whereas lemmatization reduces the word-forms to linguistically valid meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7h5pJ9tNWEJn"
   },
   "outputs": [],
   "source": [
    "# you can build your own NLP function\n",
    "# edit it according to your requirements\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def NLP_preprocess(some_text):\n",
    "  \"\"\"\n",
    "  Normalization using NLTK and spaCy\n",
    "  \"\"\"\n",
    "  # 1. Tokenization\n",
    "  NLP_token = word_tokenize(some_text)\n",
    "\n",
    "  # 2. Stemming\n",
    "  PS = PorterStemmer()\n",
    "  NLP_stem = []\n",
    "  for word in NLP_token:\n",
    "      NLP_stem.append(PS.stem(word))\n",
    "\n",
    "  # 3. Lemmatization\n",
    "  WL = WordNetLemmatizer()\n",
    "  NLP_lemma = []\n",
    "  for word in NLP_stem:\n",
    "      NLP_lemma.append(WL.lemmatize(word))\n",
    "  \n",
    "  # 4. Stopword   \n",
    "  FS = []  \n",
    "  NLP_stop = set(stopwords.words(\"english\"))\n",
    "  for w in NLP_lemma:  \n",
    "      if w not in NLP_stop:  \n",
    "        FS.append(w)\n",
    "  \n",
    "  # 5. Punctuation  \n",
    "  punctuations = \"?:!.,;\"\n",
    "  for word in FS:\n",
    "      if word in punctuations:\n",
    "          FS.remove(word)\n",
    "\n",
    "  # print comparison\n",
    "  print(\" \")\n",
    "  print(some_text)\n",
    "  print(FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yzxo1AWSWFTz"
   },
   "outputs": [],
   "source": [
    "# example\n",
    "NLP_preprocess(string_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpnCzn7Jj7XD"
   },
   "source": [
    "## Step 3: Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1kcKwXjDJ5ES"
   },
   "source": [
    "**Create new features with NLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWzId1JpmOYt"
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "CV = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-m30-9q5mwYF"
   },
   "outputs": [],
   "source": [
    "## training\n",
    "# transforming email into counts\n",
    "# counting the number of times a word appears in each email\n",
    "# 'x_train_count' is a sparse matrix, this avoids to store the zeroes\n",
    "x_train_count = CV.fit_transform(x_train)\n",
    "\n",
    "# returns: n_samples, n_features\n",
    "print(\"total emails =\", x_train_count.shape[0])\n",
    "print(\"total words =\", x_train_count.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzOt7nuUrACv"
   },
   "outputs": [],
   "source": [
    "# show the counts in train\n",
    "print(x_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrovMutirKZS"
   },
   "outputs": [],
   "source": [
    "# full matrix\n",
    "x_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yrAimS4j7XD"
   },
   "outputs": [],
   "source": [
    "## test\n",
    "# transforming email into counts\n",
    "# counting the number of times a word appears in each email\n",
    "# using the vocabulary fitted with '.fit'\n",
    "# sparse matrix: only non-zeroes elements are stored\n",
    "x_test_count = CV.transform(x_test)\n",
    "\n",
    "# returns: n_samples, n_features\n",
    "print(\"total emails =\", x_test_count.shape[0])\n",
    "print(\"total words =\", x_test_count.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcxtAqZhrt8h"
   },
   "outputs": [],
   "source": [
    "# array mapping from feature integer indices to feature name\n",
    "int2feature = CV.get_feature_names()\n",
    "print(int2feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fd50PJqPv1Np"
   },
   "outputs": [],
   "source": [
    "# warning:\n",
    "# be aware that running several times next cell\n",
    "# it will append 'Class' each time \n",
    "c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Z2mYaDqsgVI"
   },
   "outputs": [],
   "source": [
    "# append 'Class' to the end of the list\n",
    "if c==0:\n",
    "  int2feature.append('Class')\n",
    "  # print last 10 feature names\n",
    "  print(int2feature[len(int2feature)-10:len(int2feature)-1])\n",
    "  c = 1\n",
    "else:\n",
    "  print('already appended')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISTJS6Gvj7XE"
   },
   "outputs": [],
   "source": [
    "# new dataset\n",
    "new_dataset = pd.DataFrame(data=np.hstack([x_train_count.toarray(),y_train.values.reshape(-1,1)]), columns=int2feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3QXMdhIypsb"
   },
   "outputs": [],
   "source": [
    "# first rows\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqk2jRleyxd3"
   },
   "outputs": [],
   "source": [
    "new_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8RoL4Z7uk62"
   },
   "outputs": [],
   "source": [
    "# write object to a comma-separated values (csv) file\n",
    "# verify in your folder\n",
    "new_dataset.to_csv(path+\"spam_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyHnie_5j7XE"
   },
   "source": [
    "### Data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdrMNE8jj7XF"
   },
   "outputs": [],
   "source": [
    "# select an email number\n",
    "row_index = 0\n",
    "print(\"non-sparse matrix =\", x_train_count[row_index,:].todense())\n",
    "\n",
    "# original words in the email\n",
    "print('original words: ', x_train.values[row_index])\n",
    "\n",
    "# decoded numerical input \n",
    "DNI = x_train_count[row_index,:].todense()\n",
    "# inverse_transform: return terms per document with nonzero entries\n",
    "print('decoded input: ', CV.inverse_transform(DNI))\n",
    "\n",
    "# index of words\n",
    "ind = np.where(DNI[0,:]>0)[1]\n",
    "print('word indexes: ', ind)\n",
    "\n",
    "# number of times those words appears in the email\n",
    "print(x_train_count[row_index,ind].todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGilAvtbj7XF"
   },
   "source": [
    "## Step 4: Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-KYhFh859c-"
   },
   "source": [
    "Training the classifier and making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9ZrtUKej7XF"
   },
   "outputs": [],
   "source": [
    "# create a model\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# fit to data\n",
    "MNB.fit(x_train_count, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW7hoVB76Yqf"
   },
   "outputs": [],
   "source": [
    "# testing the model\n",
    "\n",
    "prediction_train = MNB.predict(x_train_count)\n",
    "print('training prediction\\t', prediction_train)\n",
    "\n",
    "prediction_test = MNB.predict(x_test_count)\n",
    "print('test prediction\\t\\t', prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcAd5Wh6j7XG"
   },
   "outputs": [],
   "source": [
    "# set_printoptions: If True, always print floating point numbers \n",
    "# using fixed point notation, in which case numbers equal to zero \n",
    "# in the current precision will print as zero\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Ham and Spam probabilities in test\n",
    "class_prob = MNB.predict_proba(x_test_count)\n",
    "print(class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqFxSVTzj7XG"
   },
   "outputs": [],
   "source": [
    "# show emails classified as 'spam'\n",
    "threshold = 0.5\n",
    "spam_ind = np.where(class_prob[:,1]>threshold)[0]\n",
    "print(x_test.values[spam_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkIb-_wUj7XG"
   },
   "source": [
    "## Step 5: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BjreL3sdj7XH"
   },
   "outputs": [],
   "source": [
    "# accuracy in training set\n",
    "y_pred_train = prediction_train\n",
    "print(\"Train Accuracy: \"+str(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQRTS8rTj7XH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# accuracy in test set (unseen data)\n",
    "y_true = y_test\n",
    "y_pred_test = prediction_test\n",
    "print(\"Test Accuracy: \"+str(accuracy_score(y_true, y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yqg3jqtWAPJB"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred_test)\n",
    "print(\"Confusion Matrix\\n\", conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypc8HXCYEpNz"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "labels = ['Ham','Spam']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(conf_mat)\n",
    "plt.title('Confusion matrix of the classifier\\n')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ET5003_NLP_SpamClassifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
